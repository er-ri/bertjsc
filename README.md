# BERTJSC
Japanese Spelling Error Corrector using BERT(Masked-Language Model).

æ—¥æœ¬èªã¯[ã“ã¡ã‚‰](#ã¯ã˜ã‚ã«)ã§ã™

## Abstract
The project, fine-tuned the Masked-Language BERT for the task of Japanese Spelling Error Correction. The whole word masking pretrained [model](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking) has been applied in the project. Besides the original BERT, another architecture based on BERT used a method called [soft-masking](https://arxiv.org/abs/2005.07421) which introducing a **Bi-GRU** layer to the original BERT architecture, was also implemented in this project. The training dataset(Japanese Wikipedia Typo Dataset version 1) contains a pair of grammatical error and error-free sentence which is collected from wikipedia, can be downloaded at [here](https://nlp.ist.i.kyoto-u.ac.jp/EN/edit.php?JWTD).

## Getting Started
1. Clone the project and install necessary packages.  
    pip install -r requirements.txt
2. Download the fine-tuned model(BERT) from [here](https://drive.google.com/file/d/1IVcwz70GWWpOfJNG-Jm4jKjA9ObcrHJY/view?usp=sharing) and put it to an arbitrary directory.
3. Make a inference by the following code.
```python
    import torch
    from transformers import BertJapaneseTokenizer
    from bertjsc import predict_of_json 
    from bertjsc.lit_model import LitBertForMaskedLM

    # Tokenizer & Model declaration.
    tokenizer = BertJapaneseTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
    model = LitBertForMaskedLM("cl-tohoku/bert-base-japanese-whole-word-masking")

    # Load the model downloaded in Step 2. 
    model.load_state_dict(torch.load('load/from/path/lit-bert-for-maskedlm-230313.pth'))

    # Set computing device on GPU if available, else CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Inference
    result = predict_of_json(model, tokenizer, device, "æ—¥æœ¬èªæ ¡æ­£ã—ã¦ã¿ã™ã€‚")
    print(result) 
```
4. A json style result will be displayed as below.
```json
{
    0: {'token': 'æ—¥æœ¬èª', 'score': 0.999341},
    1: {'token': 'æ ¡', 'score': 0.996382},
    2: {'token': 'æ­£', 'score': 0.997387},
    3: {'token': 'ã—', 'score': 0.999978},
    4: {'token': 'ã¦', 'score': 0.999999},
    5: {'token': 'ã¿', 'score': 0.999947},
    6: {'token': 'ã™', 'correct': 'ã¾ã™', 'score': 0.972711},
    7: {'token': 'ã€‚', 'score': 1.0}
}
```
* For training the model from scratch, you need to download the training data from [here](https://nlp.ist.i.kyoto-u.ac.jp/EN/edit.php?JWTD). The file(`./scripts/trainer.py`) contains the steps for the training process and includes a function to evaluate model's performance. You may refer the file to perform your task on GPU cloud computing platform like `AWS SageMaker` or `Google Colab`.
* For using `Soft-Masked BERT`, download the fine-tuned model from [here](https://drive.google.com/file/d/1uZQWq4gNszhmpFijNGHY8DB9ppuzHS7t/view?usp=sharing), declare the model as the following code. The other usages are the same.
```python
    from transformers import BertJapaneseTokenizer
    from bertjsc.lit_model import LitSoftMaskedBert
    
    tokenizer = BertJapaneseTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
    model = LitSoftMaskedBert("cl-tohoku/bert-base-japanese-whole-word-masking", tokenizer.mask_token_id, tokenizer.vocab_size)
```

## Evaluation
### Detection
| Model | Accuracy | Precision | Recall | F1 Score |
|---|---|---|---|---|
| Pre-train   |   43.5   |   31.9   |   41.5   |   36.1   |
| Fine-tune   |   77.3   | **71.1** |   81.2   | **75.8** |
| Soft-masked | **78.4** |   65.3   | **88.4** |   75.1   |

### Correction
| Model | Accuracy | Precision | Recall | F1 Score |
|---|---|---|---|---|
| Pre-train   |   37.0   |   19.0   |   29.8   |   23.2   |
| Fine-tune   |   74.9   | **66.4** |   80.1   | **72.6** |
| Soft-masked | **76.4** |   61.4   | **87.8** |   72.2   |
* Training Platform: *AWS SageMaker Lab*
* Batch Size: *32*
* Epoch: *6*
* Learning Rate: *2e-6*
* Coefficient(soft-masked): *0.8*

## License
Distributed under the MIT License. See `LICENSE.txt` for more information.

## Contribution
Any suggestions for improvement or contribution to this project are appreciated! Feel free to submit an issue or pull request!

## References
1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
3. [Spelling Error Correction with Soft-Masked BERT](https://arxiv.org/abs/2005.07421)
4. [HuggingFaceğŸ¤—: BERT base Japanese](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)
5. [pycorrector](https://github.com/shibing624/pycorrector)
6. [åŸºäºBERTçš„æ–‡æœ¬çº é”™æ¨¡å‹](https://github.com/gitabtion/BertBasedCorrectionModels)
7. [SoftMasked Bertæ–‡æœ¬çº é”™æ¨¡å‹å®ç°](https://github.com/quantum00549/SoftMaskedBert)

---

## ã¯ã˜ã‚ã«
1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚  
    pip install -r requirements.txt
2. [ã“ã“](https://drive.google.com/file/d/1SiRPOnjoDfK-N2sTEBUlGX22vVo4Pif1/view?usp=sharing)ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚
3. æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã§æ¨è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
```python
    import torch
    from transformers import BertJapaneseTokenizer
    from bertjsc import predict_of_json 
    from bertjsc.lit_model import LitBertForMaskedLM

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®å®£è¨€
    tokenizer = BertJapaneseTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
    model = LitBertForMaskedLM("cl-tohoku/bert-base-japanese-whole-word-masking")

    # ã‚¹ãƒ†ãƒƒãƒ—2ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚
    model.load_state_dict(torch.load('load/from/path/lit-bert-for-maskedlm-230112.pth'))

    # GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯GPUä¸Šã§è¨ˆç®—ã‚’è¡Œã„ã€ãã‚Œä»¥å¤–ã®å ´åˆã¯CPUä¸Šã§è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ¨è«–
    result = predict_of_json(model, tokenizer, device, "æ—¥æœ¬èªæ ¡æ­£ã—ã¦ã¿ã™ã€‚")
    print(result) 
```
4. ä»¥ä¸‹ã®ã‚ˆã†ãªjsonå½¢å¼ã®çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
```json
{
    0: {'token': 'æ—¥æœ¬èª', 'score': 0.999341},
    1: {'token': 'æ ¡', 'score': 0.996382},
    2: {'token': 'æ­£', 'score': 0.997387},
    3: {'token': 'ã—', 'score': 0.999978},
    4: {'token': 'ã¦', 'score': 0.999999},
    5: {'token': 'ã¿', 'score': 0.999947},
    6: {'token': 'ã™', 'correct': 'ã¾ã™', 'score': 0.972711},
    7: {'token': 'ã€‚', 'score': 1.0}
}
```
* ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€[ã“ã“](https://nlp.ist.i.kyoto-u.ac.jp/EN/edit.php?JWTD)ã‹ã‚‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«(`./scripts/trainer.py`)ã«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®æ‰‹é †ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚`AWS SageMaker`ã‚„`Google Colab`ãªã©ã®GPUã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
* `Soft-Masked BERT`ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€[ã“ã“](https://drive.google.com/file/d/1uZQWq4gNszhmpFijNGHY8DB9ppuzHS7t/view?usp=sharing)ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’å®£è¨€ã—ã¦ãã ã•ã„ã€‚ãã®ä»–ã®ä½¿ç”¨æ–¹æ³•ã¯åŒã˜ã§ã™ã€‚
```python
from bertjsc.lit_model import LitSoftMaskedBert
model = LitSoftMaskedBert("cl-tohoku/bert-base-japanese-whole-word-masking", tokenizer.mask_token_id, tokenizer.vocab_size)
```